<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="author" content="Charles Jekel">
  <meta name="description" content="A lack of fit test is performed for simple linear regression model to see if the use of a linear model is appropriate for the given data set">
  
  <meta name="keywords" content="lack of fit test,lack of fit,linear regression,Python">
  
  
  <title>Charles Jekel - jekel.me - Lack of Fit Test for Linear Regression</title>
  <link rel="shortcut icon" href="/assets/images/favicon.ico">
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="alternate" type="application/rss+xml" title="My Blog" href="/feed.xml">
  <link rel="stylesheet" href="/assets/css/highlight.css">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>

  <nav class="main-nav">
    
        <a href="/"> <span class="arrow">←</span> Home </a>
    

    
            <a href="/about">About </a>
    

    
            <a href="/cv">CV </a>
    
    <a class="cta" href="/feed.xml">Subscribe</a>
</nav>


  

  <section id="wrapper" class="">
    <article class="post">
    <header>
   
        <h1>Lack of Fit Test for Linear Regression</h1>
        <h2 class="headline">March 18, 2017</h2>
    </header>
    <section id="post-body">
        <p>Myers et al. [1] provides a derivation for a simple lack of fit test in section 2.7. The derivation  looks at whether or not a linear model of form</p>

<div>
$$
\hat{y} = \beta_0 + \beta_1 x
$$
</div>

<p>adequately describes the data. Unfortunately the example provided has a mistake (while the figure matches the data, the coefficients and error values do not), so this post aims to provide corrected error values for the sample data. In addition, all of the source code to work through the problem in Python is provided <a href="/assets/2017-03-18/linExample.py">here</a>.</p>

<p>In order to apply the lack of fit test, we’ll need a data set that has true replicates as described in [1]. The following figure contains the data set we’ll be working with, along with the linear model fit to the data.</p>

<p><img src="/assets/2017-03-18/dataAndFit.png" alt="A linear model is fit to the data set." /></p>

<p>The coefficients of the linear model that were determined with a least squares fit are <span>\( \beta_0 =13.25847151,  \beta_1 = 2.12367129  \)</span>. The Python code to generate the data, create the figure, and perform the least squares fit is available below.</p>

<div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="c">#   Create the data set</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">,</span>
    <span class="mf">5.6</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="mf">6.92</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">10.84</span><span class="p">,</span> <span class="mf">9.30</span><span class="p">,</span> <span class="mf">16.35</span><span class="p">,</span> <span class="mf">22.88</span><span class="p">,</span> <span class="mf">24.35</span><span class="p">,</span> <span class="mf">24.56</span><span class="p">,</span> <span class="mf">25.86</span><span class="p">,</span> <span class="mf">29.46</span><span class="p">,</span>
    <span class="mf">24.59</span><span class="p">,</span> <span class="mf">22.25</span><span class="p">,</span> <span class="mf">25.90</span><span class="p">,</span> <span class="mf">27.20</span><span class="p">,</span> <span class="mf">25.61</span><span class="p">,</span> <span class="mf">25.45</span><span class="p">,</span> <span class="mf">26.56</span><span class="p">,</span> <span class="mf">21.03</span><span class="p">,</span> <span class="mf">21.46</span><span class="p">])</span>

<span class="c">#   fit a linear model</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">A</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">beta</span><span class="p">,</span> <span class="n">SSe</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">xl</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">Al</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">xl</span><span class="p">),</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Al</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">xl</span>
<span class="n">yl</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Al</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
<span class="c">#   plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s">'ok'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xl</span><span class="p">,</span><span class="n">yl</span><span class="p">,</span><span class="s">'-b'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'$x$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'$y$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'dataAndFit.png'</span><span class="p">,</span> <span class="n">ftype</span><span class="o">=</span><span class="s">'png'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span></code></pre></figure>

</div>

<p>As it happens, the data has <span>\( n_i \) </span> observations for each unique <span>\( x_i\)</span> level. For instance when <span>\( x = 1.0 \)</span>, there are 2 observations of <span>\(x\)</span>. However <span>\( x= 2.0 \)</span> only has 1 observation. The data has <span>\( m \)</span> number of levels. The residual sum of squares (the sum of squares of the residuals, in Python variable SSe returned from the linear regression) is then broken into two separate components such that</p>

<div>
$$
SS_E = SS_{PE} + SS_{LOF}
$$
</div>
<p>where <span>\( SS_{PE} \)</span> represents the sum of squares from pure error and <span>\( SS_{LOF} \)</span> represents the sum of squares from the lack of fit. The sum of squares from pure error is defined as</p>
<div>
$$
SS_{PE} = \sum_{i=1}^m \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2
$$
</div>
<p>where <span>\( y_{ij} \)</span> is the <em>j</em> observation from the data at the <span>\( x_i \) level where <span>\( j = 1, \cdots, n_i \)</span>. Source code to calculate the sum of the squares due to pure error is provided below.</span></p>

<div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#   compute the sum of squares of pure error</span>
<span class="n">level</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="mf">6.92</span><span class="p">])</span>
<span class="n">levelIndex</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">9</span><span class="p">],</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">13</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">13</span><span class="p">:</span><span class="mi">15</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">15</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">16</span><span class="p">]]</span>
<span class="n">ybarLevels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">levelIndex</span><span class="p">:</span>
    <span class="n">ybarLevels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="n">SSpe</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">levelIndex</span><span class="p">):</span>
    <span class="n">SSpe</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">r</span><span class="o">-</span><span class="n">ybarLevels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span></code></pre></figure>

</div>

<p>The sum of squares from lack of fit is defined as</p>
<div>
$$
SS_{LOF} = \sum_{i=1}^m n_i(\bar{y}_i - \hat{y}_i)^2
$$
</div>
<p>where <span>\( \bar{y}\)</span> is the <em>y</em> mean of the <em>i</em> level and <span>\( \hat{y}\)</span> is the predicted <em>y</em> response at <span>\( x_i \)</span>. The source code to calculate the sum of squares for the lack of fit is provided below.</p>

<div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#   compute the sum of squares lack of fit</span>
<span class="n">nl</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">level</span><span class="p">)</span>
<span class="n">SSlof</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Alevel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">nl</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">Alevel</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">level</span>
<span class="n">yhatLevel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Alevel</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ybarLevels</span><span class="p">):</span>
    <span class="n">ni</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">levelIndex</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">SSlof</span><span class="o">+=</span> <span class="n">ni</span><span class="o">*</span><span class="p">((</span><span class="n">j</span><span class="o">-</span><span class="n">yhatLevel</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span></code></pre></figure>

</div>

<p>We thus far have for this example that <span>\(SS_{E} = 255.22, SS_{PE} = 17.20, SS_{LOF} = 238.02 \)</span>. It can be noted that the equation <span>\( SS_E = SS_{PE} + SS_{LOF}
 \)</span> holds true.</p>

<p>The test statistic <span>\( F_0 \)</span> to test for lack of fit is defined as</p>
<div>
 $$
F_0 = \frac{SS_{LOF}/(m-p)}{SS_{PE}/(n-m)}
 $$
 </div>
<p>for <span>\( p \)</span> number of parameters. The code to calculate the test statistic is provided below.</p>
<div>
 
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#   Statistical test for lack of fit</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ybarLevels</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="n">F0</span> <span class="o">=</span> <span class="p">(</span><span class="n">SSlof</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="n">p</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">SSpe</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">m</span><span class="p">))</span>

 </code></pre></figure>

 </div>

<p>We find that <span>\( F_0 = 12.106 \)</span> which follows the F-distribution of <span>\( F(\alpha : m-p, n-m) \)</span>. It can be concluded that regression function is not linear because <span>\( F_0 = 12.106 &gt; F(\alpha : m-p, n-m) \)</span>. Alternatively the P-value for the hypothesis of <span>\( P(F_0 \leq F(F_0: m-p, n-m) \)</span> is 0.0018. Since the P-value is so small, the hypothesis is rejected. The P-value can be calculated with the following code:</p>
<div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#   test for lack of fit</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">f</span>
<span class="n">pValue</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">f</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">F0</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="n">p</span><span class="p">,</span><span class="n">n</span><span class="o">-</span><span class="n">m</span><span class="p">)</span></code></pre></figure>

</div>

<p>In conclusion the linear model of <span>\( \hat{y} = \beta_0 + \beta_1 x
 \)</span> is inadequate of representing the data because most of the error results from the lack of fit, rather than pure error in the data. A better model can be constructed (potentially a higher order polynomial) to reduce the error from lack of fit.</p>

<p>[1] R. H. Myers, D. C. Montgomery, C. M. Anderson-Cook, <em>Response Sur-
face Methodology: Process and Product Optimization Using Designed
Experiments</em>, Wiley Series in Probability and Statistics, Wiley, ISBN
9781118916032, 2016.</p>

    </section>
</article>
<footer id="post-meta" class="clearfix">
    <a href="/about">
        <img class="avatar" src="/assets/images/avatar.png">
        <div>
            <span class="dark">Charles Jekel</span>
            <span>Mechanical Engineer. Numerical Modeling. Optimization.</span>
            <span>cj@jekel.me</span>

            <span><a href="/">Home </a><a href="/cv"> CV </a></span>
        </div>
    </a>

    <!-- Remove Sharing links for now:<section id="sharing">
        <a class="twitter" href="https://twitter.com/intent/tweet?text=http://jekel.me/2017/Lack-of-fit-test-linear-regression/ - Lack of Fit Test for Linear Regression by @"><span class="icon-twitter"> Tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>
    </section>-->
</footer>

<!-- Disqus comments -->

    <div class="archive readmore">
        <h3>Comments</h3>
        <section class="disqus">
    <div id="disqus_thread"></div>
    <script>
        var disqus_config = function () {
            this.page.url = 'http://localhost:4000/2017/Lack-of-fit-test-linear-regression/';
            this.page.identifier = '/2017/Lack-of-fit-test-linear-regression';
        };
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = '//cjekel.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>

    </div>


<!-- Archive post list -->





  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script src="/assets/js/main.js"></script>
  <script src="/assets/js/highlight.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67542734-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>



